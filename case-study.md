# Case-study оптимизации

## Актуальная проблема
Необходимо обработать файл с данными, больше ста двадцати мегабайт.

На старте уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала бесконечно долго.

Было решено исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли изменения программы положительный эффект на быстродействие программы принято решение использовать такую метрику: *время выполнения программы*

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы поставлена задача выстроить эффективный `feedback-loop`, который позволит получать обратную связь по эффективности сделанных изменений за *финальное время после оптимизации*

Для построения фидбек-лупа возьмём объём данных в размере 1000 срок

Оценка ассимптотики (ruby benchmark_task.rb)
```table
size       1X      2X      3X      4X      5X  (1000 строк) 
time      0.35    1.07    2.01    3.24    4.81 (секунд)
delta     0.35    0.72    0.94    1.23    1.57 (секунд/1000строк)
accel      1      1.06    0.63    0.82    0.97 (доля=секунд/1000строк^2)
```
mean(delta) ~ 96% 

mean(acceleration) ~ 90% 

Оценка продолжительности обработки полного объёма данных для исходной версии программы
```equation
3250940 / 1000 * mean(delta) ~ 52 минуты
```
Вот как выглядит `feedback_loop`: 
1. Profile `ruby-prof`: `ruby task.rb`
2. Modify code
3. Test: `ruby test-task.rb`
4. Benchmark: `ruby benchmark_task.rb`
5. Add case-study
6. Commit

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации воспользуемся 
* gem benchmark
* профилировщик `ruby-prof`

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- отчёт `ruby_prof_reports/flat0.txt`  показал главную точку роста
```table
 44.93      1.442     1.442     0.000     0.000      154   Array#select   
```
- в качестве оптимизации принято решение поменять `sessions = Array` на `Hash` с ключами по `user_id`
- метрика `1X` уменьшилась до `0.15`
- в итоге в отчёте профилировщика "точка роста" с `Array#select` исчезла 

### Ваша находка №2
- отчёт `ruby_prof_reports/flat1.txt`  показал главную точку роста
```table
 51.95      0.880     0.878     0.000     0.002     1000   Array#all?
```
- в качестве оптимизации принято решение переписать конструктор `uniqueBrowsers`
- метрика `1X` уменьшилась до `0.1`
- в итоге в отчёте профилировщика "точка роста" с `Array#all?` исчезла 

### Ваша находка №3
- отчёт `ruby_prof_reports/flat2.txt`  показал главную точку роста
```table
 27.36      0.817     0.227     0.000     0.589       10   Array#each
```
- в качестве оптимизации принято решение внести все очевидные оптимизации внутри циклов each
- метрика `1X` уменьшилась до `0.__`
- в итоге в отчёте профилировщика "точка роста" с `Array#each` перестала быть ведущей 

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с ориентировочно 50 минут до XX секунд и уложиться в заданный бюджет.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы были добавлены RSpec-тесты `assert-performance.rb`

